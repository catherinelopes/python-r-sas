---
title: "Python Tutorial"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, engine = 'python', engine.path="/Users/elaineek/anaconda/bin/python")
```

First, you need to import several important Python packages for data manipulation and scientific computing.  The [pandas](#PANDAS) package is helpful for data manipulation and the [NumPy](#NUMPY) package is helpful for scientific computing. 
```{python, eval = FALSE}
import pandas as pd
import numpy as np
```

In Python, comments are indicated in code with a "#" character, and arrays and matrices are zero-indexed.

# 1 Reading in Data and Basic Statistical Functions

## 1.1 Read in the data.

The following demonstrate importing data in Python given 3 different data formats.  The pandas package is able to read all 3 formats, as well as many others, using [Python IO tools](http://pandas.pydata.org/pandas-docs/version/0.20/io.html). 

### a) Read the data in as a .csv file.

```{python, eval = FALSE}
student = pd.read_csv('/Users/class.csv')
```

### b) Read the data in as a .xls file.

```{python, eval = FALSE}
# Notice you must specify the file location, as well as the name of the sheet 
# of the .xls file you want to import
student_xls = pd.read_excel(open('/Users/class.xls', 'rb'), 
                            sheetname='class')
```

### c) Read the data in as a .json file.

```{python, eval = FALSE}
student_json = pd.read_json('/Users/class.json')
```

## 1.2 Find the dimensions of the data set.

The dimensions of a [DataFrame](#DataFrame) in Python are known as an attribute of the object.  Therefore, you can state the data name followed by ".shape" to return the dimensions of the data.

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student.shape)
```

## 1.3 Find basic information about the data set.

Information about a [DataFrame](#DataFrame) is available by calling the ".info()" function on the data.

```{python, echo=3:4}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
# Notice that student is a DataFrame object
print(student.info())
```

## 1.4 Look at the first 5 observations.

The first 5 observations of a [DataFrame](#DataFrame) are available by calling the ".head()" function on the data.  By default, head() returns 5 observations.  To return the first *n* observations, pass the integer *n* into the function.  The tail() function is analogous and returns the last observations.

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student.head())
```

## 1.5 Calculate mean of numeric variables.

```{python, echo=3:5}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
# By default, the mean() function returns the mean of numeric variables of 
# the data only
print(student.mean())
```

## 1.6 Compute summary statistics of the data set.

Summary statistics of a [DataFrame](#DataFrame) are available by calling the ".describe()" function on the data.

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student.describe())
```

## 1.7 Descriptive statistics functions applied to variables of the data set.

```{python, echo=3:5}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
# Notice the subsetting of student with [] and the name of the variable in 
# quotes 
print(student["Weight"].std())
```

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student["Weight"].sum())
```

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student["Weight"].count())
```

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student["Weight"].max())
```

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student["Weight"].min())
```

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student["Weight"].median())
```

## 1.8 Produce a one-way table to describe the frequency of a variable.

### a) Produce a one-way table of a discrete variable.
```{python, echo=3:5}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
# columns = "count" indicates to make the descriptive portion of the table 
# the counts of each level of the index variable
print(pd.crosstab(index=student["Age"], columns="count"))
```

### b) Produce a one-way table of a categorical variable.
```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(pd.crosstab(index=student["Sex"], columns="count"))
```

## 1.9 Produce a two-way table to describe the frequency of two categorical or discrete variables.

```{python, echo=3:4}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
# Notice the specification of a variable for the columns argument, instead 
# of "count"
print(pd.crosstab(index=student["Age"], columns=student["Sex"]))
```
[crosstab()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html)

## 1.10 Select a subset of the data that meets a certain criterion.

```{python, echo=3:4}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
females = student.query('Sex == "F"')
print(females.head())
```
[query()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html)

## 1.11 Determine the correlation between two continuous variables.

```{python, echo = 3:4}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
height_weight = pd.concat([student["Height"], student["Weight"]], axis = 1)
print(height_weight.corr(method = "pearson"))
```
[corr()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html)

***

\newpage

# 2 Basic Graphing and Plotting Functions

The Matplotlib [PyPlot](#PYPLOT) package is a standard Python package to use for plotting.  For more information on other Python plotting packages, please see the Appendix Section 2.

```{python}
import matplotlib.pyplot as plt
```

## 2.1 Visualize a single continuous variable by producing a histogram.

```{python, eval=FALSE}
# Notice how the bin endpoints are set so the histogram is the same
# as that produced by SAS and R
# Also notice the labeling of the axes
plt.hist(student["Weight"], bins=[40,60,80,100,120,140,160])
plt.xlabel('Weight')
plt.ylabel('Frequency')
plt.show()
```

Output:
![output](histogram2.png)

## 2.2 Visualize a single continuous variable by producing a boxplot.

```{python, eval=FALSE}
# showmeans=True tells Python to plot the mean of the variable on the boxplot 
plt.boxplot(student["Weight"], showmeans=True)
 # prevents Python from printing a "1" at the bottom of the boxplot
plt.xticks([]) # prevents Python from printing a "1" at the bottom of the boxplot
plt.ylabel('Weight')
plt.show()
```

Output:
![output](boxplot.png)

## 2.3 Visualize two continuous variables by producing a scatterplot.

```{python, eval=FALSE}
# Notice here you specify the x variable first followed by the y variable 
plt.scatter(student["Height"], student["Weight"])
plt.xlabel("Height")
plt.ylabel("Weight")
plt.show()
```

Output:
![output](scatter.png)

## 2.4 Visualize a relationship between two continuous variables by producing a scatterplot and a plotted line of best fit.

```{python, eval=FALSE}
x = student["Height"]
y = student["Weight"]

# np.polyfit() models Weight as a function of Height and returns the 
# parameters
m, b = np.polyfit(x, y, 1)
plt.scatter(x, y)

# plt.text() prints the equation of the line of best fit, with the first two 
# arguments specifying the x and y locations of the text, respectively 
# %f indicates to print a floating point number, that is specified following
# the string
plt.text(51, 140, "Line: y = %f x + %f"% (m,b))
plt.plot(x, m*x + b)
plt.xlabel("Height")
plt.ylabel("Weight")
plt.show()
```

Output:
![output](lineofbestfit.png)

[NumPy polyfit()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html)

## 2.5 Visualize a categorical variable by producing a bar chart.

```{python, eval=FALSE}
# Get the counts of Sex 
counts = pd.crosstab(index=student["Sex"], columns="count")

# len() returns the number of categories of Sex (2)
# np.arange() creates a vector of the specified length
num = np.arange(len(counts))
plt.bar(num,counts["count"], align='center', alpha=0.5)

# Set the xticks to be the indices of counts
plt.xticks(num, counts.index)
plt.xlabel("Sex")
plt.ylabel("Frequency")
plt.show()
```

Output:
![output](barchart.png)

[NumPy arange()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.arange.html)

## 2.6 Visualize a continuous variable, grouped by a categorical variable, by producing side-by-side boxplots.

### a) Simple side-by-side boxplot without color.
```{python, eval=FALSE}
# Subset data set to return only female weights, and then only male weights 
Weight_F = np.array(student.query('Sex == "F"')["Weight"])
Weight_M = np.array(student.query('Sex == "M"')["Weight"])
Weights = [Weight_F, Weight_M]

# PyPlot automatically plots the two weights side-by-side since Weights 
# is a 2D array
plt.boxplot(Weights, showmeans=True, labels=('F', 'M'))
plt.xlabel('Sex')
plt.ylabel('Weight')
plt.show()
```

Output:
![output](sboxplot.png)

### b) More advanced side-by-side boxplot with color.

```{python, eval=FALSE}
import seaborn as sns
sns.boxplot(x="Sex", y="Weight", hue="Sex", data = student, showmeans=True)
plt.show()
```
[seaborn boxplot](http://seaborn.pydata.org/generated/seaborn.boxplot.html)

[seaborn](#SEABORN)

Output:
![output](snsboxplot.png)

***

\newpage

# 3 Basic Data Wrangling and Manipulation

## 3.1 Create a new variable in a data set as a function of existing variables in the data set.

```{python, echo=3:7}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
# Notice here how you can create the BMI column in the 
# data set just by naming it 

student["BMI"] = student["Weight"] / student["Height"]**2 * 703
print(student.head())
```

## 3.2 Create a new variable in a data set using if/else logic of existing variables in the data set.

```{python, echo=5:8}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
# Notice the use of the np.where() function for a single condition 
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
print(student.head())
```

[NumPy where()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html)

## 3.3 Create new variables in a data set using mathematical functions applied to existing variables in the data set.

Using the log() function, the exp() function, the sqrt() function, and the abs() function.
```{python, echo=6:14}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
student["LogWeight"] = np.log(student["Weight"])
student["ExpAge"] = np.exp(student["Age"])
student["SqrtHeight"] = np.sqrt(student["Height"])
student["BMI Neg"] = np.where(student["BMI"] < 19.0, -student["BMI"], student["BMI"])
student["BMI Pos"] = np.abs(student["BMI Neg"])

# Create a boolean variable
student["BMI Check"] = (student["BMI Pos"] == student["BMI"])
print(student.head())
```

## 3.4 Drop variables from a data set.

```{python, echo=12:15}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
student["LogWeight"] = np.log(student["Weight"])
student["ExpAge"] = np.exp(student["Age"])
student["SqrtHeight"] = np.sqrt(student["Height"])
student["BMI Neg"] = np.where(student["BMI"] < 19.0, -student["BMI"], student["BMI"])
student["BMI Pos"] = np.abs(student["BMI Neg"])
student["BMI Check"] = (student["BMI Pos"] == student["BMI"])
# axis = 1 indicates to drop columns instead of rows
student = student.drop(["LogWeight", "ExpAge", "SqrtHeight", "BMI Neg", 
                        "BMI Pos", "BMI Check"], axis = 1)
print(student.head())
```

## 3.5 Sort a data set by a variable.

### a) Sort data set by a continuous variable.

```{python, echo=6:9}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
# Notice the kind="mergesort" which indicates to use a stable sorting 
# algorithm 
student = student.sort_values(by="Age", kind = "mergesort")
print(student.head())
```

### b) Sort data set by a categorical variable.

```{python, echo=7:9}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
student = student.sort_values(by="Age", kind = "mergesort")
student = student.sort_values(by="Sex", kind = "mergesort")
# Notice that the data is now sorted first by Sex and then within Sex by Age 
print(student.head())
```

## 3.6 Compute descriptive statistics of continuous variables, grouped by a categorical variable.

```{python, echo=8}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
student = student.sort_values(by="Age", kind = "mergesort")
student = student.sort_values(by="Sex", kind = "mergesort")
print(student.groupby(by = "Sex").mean())
```

## 3.7 Add a new row to the bottom of a data set.

```{python, echo=8:9}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
student = student.sort_values(by="Age", kind = "mergesort")
student = student.sort_values(by="Sex", kind = "mergesort")
# Look at the tail of the data currently
print(student.tail())
```

```{python, echo=8:16}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
student = student.sort_values(by="Age", kind = "mergesort")
student = student.sort_values(by="Sex", kind = "mergesort")
student = student.append({'Name':'Jane', 'Sex':'F', 'Age':14, 'Height':56.3, 
                          'Weight':77.0, 'BMI':17.077695, 
                          'BMI Class': 'Underweight'}, 
                         ignore_index=True)

# Notice the change in the indices because of the ignore_index=True option 
# which allows for a Series, or one-dimensional DataFrame, to be appended 
# to an existing DataFrame 

print(student.tail())
```

## 3.8 Create a user defined function and apply it to a variable in the data set to create a new variable in the data set.

```{python, echo=12:16}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
student = student.sort_values(by="Age", kind = "mergesort")
student = student.sort_values(by="Sex", kind = "mergesort")
student = student.append({'Name':'Jane', 'Sex':'F', 'Age':14, 'Height':56.3, 
                          'Weight':77.0, 'BMI':17.077695, 
                          'BMI Class': 'Underweight'}, 
                         ignore_index=True)
def toKG(lb):
    return (0.45359237 * lb)

student["Weight KG"] = student["Weight"].apply(toKG)
print(student.head())
```
[apply()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html)

## 3.9 Caste a [Data Frame](#DataFrame) to a different object type.

```{python, echo=4}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student_num = pd.concat([student["Age"], student["Height"], 
                         student["Weight"]], axis = 1)
student_float = student_num.astype(float)
print(student_float.head())
```
[astype()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.astype.html)

***

\newpage

# 4 More Advanced Data Wrangling

## 4.1 Drop observations with missing information.

```{python, echo=2:8}
import pandas as pd
# Notice the use of the fish data set because it has some missing 
# observations 
fish = pd.read_csv('/Users/fish.csv')

# First sort by Weight, requesting those with NA for Weight first 
fish = fish.sort_values(by='Weight', kind='mergesort', na_position='first')
print(fish.head())
```

--

```{python, echo=4:5}
import pandas as pd
fish = pd.read_csv('/Users/fish.csv')
fish = fish.sort_values(by='Weight', kind='mergesort', na_position='first')
new_fish = fish.dropna()
print(new_fish.head())
```

[dropna()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html)

## 4.2 Merge two data sets together on a common variable.

### a) First, select specific columns of a data set to create two smaller data sets.

```{python, echo=2:8}
import pandas as pd
# Notice the use of the student data set again, however we want to reload it
# without the changes we've made previously 
student = pd.read_csv('/Users/class.csv')
student1 = pd.concat([student["Name"], student["Sex"], student["Age"]], 
                    axis = 1)
print(student1.head())
```

--

```{python, echo=3:5}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
student2 = pd.concat([student["Name"], student["Height"], student["Weight"]], 
                    axis = 1)
print(student2.head())
```

### b) Second, we want to merge the two smaller data sets on the common variable.

```{python, echo=7:8}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
student1 = pd.concat([student["Name"], student["Sex"], student["Age"]], 
                    axis = 1)
student2 = pd.concat([student["Name"], student["Height"], student["Weight"]], 
                    axis = 1)
new = pd.merge(student1, student2, on="Name")
print(new.head())
```

### c) Finally, we want to check to see if the merged data set is the same as the original data set.

```{python, echo=8}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
student1 = pd.concat([student["Name"], student["Sex"], student["Age"]], 
                    axis = 1)
student2 = pd.concat([student["Name"], student["Height"], student["Weight"]], 
                    axis = 1)
new = pd.merge(student1, student2, on="Name")
print(student.equals(new))
```

[merge()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html)

## 4.3 Merge two data sets together by index number only.

### a) First, select specific columns of a data set to create two smaller data sets.

```{python, echo=3:5}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
newstudent1 = pd.concat([student["Name"], student["Sex"], student["Age"]], 
                    axis = 1)
print(newstudent1.head())
```

```{python, echo=3:4}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
newstudent2 = pd.concat([student["Height"], student["Weight"]], axis = 1)
print(newstudent2.head())
```

### b) Second, we want to join the two smaller data sets.

```{python, echo=7:8}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
newstudent1 = pd.concat([student["Name"], student["Sex"], student["Age"]], 
                    axis = 1)
newstudent2 = pd.concat([student["Height"], student["Weight"]], 
                    axis = 1)
new2 = newstudent1.join(newstudent2)
print(new2.head())
```

### c) Finally, we want to check to see if the joined data set is the same as the original data set.

```{python, echo=8}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
newstudent1 = pd.concat([student["Name"], student["Sex"], student["Age"]], 
                    axis = 1)
newstudent2 = pd.concat([student["Height"], student["Weight"]], 
                    axis = 1)
new2 = newstudent1.join(newstudent2)
print(student.equals(new2))
```

[join()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html)

## 4.4 Create a pivot table to summarize information about a data set.

```{python, echo=3:17}
import pandas as pd
import numpy as np
# Notice we are using a new data set that needs to be read into the 
# environment 
price = pd.read_csv('/Users/price.csv')

# The following code is used to remove the ',' and '$' characters from 
# the ACTUAL colum so that the values can be summed 
from re import sub
from decimal import Decimal
def trim_money(money):
    return(float(Decimal(sub(r'[^\d.]', '', money))))

price["REVENUE"] = price["ACTUAL"].apply(trim_money)
table = pd.pivot_table(price, index=["COUNTRY", "STATE", "PRODTYPE", 
                                     "PRODUCT"], values="REVENUE", aggfunc=np.sum)
print(table.head())
```
[sub](#SUB)

[Decimal](#DECIMAL) 

[pivot_table()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.pivot_table.html) for more information on the pandas pivot_table() function

[pivot()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html)

***

\newpage

# 5 Regression & Modeling

The following sections focus on the Python [sklearn](#SKLEARN) package.  

## 5.1 Pre-process a data set using principal component analysis.

```{python, echo = 3:12}
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA

# Notice we are using a new data set that needs to be read into the 
# environment 
iris = pd.read_csv('/Users/iris.csv')
features = iris.drop(["Target"], axis = 1)

pca = PCA(n_components = 4)
pca = pca.fit(features)
print(np.transpose(pca.components_))
```
[PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)

## 5.2 Split data into training and testing data and export as a .csv file.

```{python, eval = FALSE}
from sklearn.model_selection import train_test_split

target = iris["Target"]

# The following code splits the iris data set into 70% train and 30% test
X_train, X_test, Y_train, Y_test = train_test_split(features, target, 
                                                    test_size = 0.3, 
                                                    random_state = 29)
train_x = pd.DataFrame(X_train)
train_y = pd.DataFrame(Y_train)
test_x = pd.DataFrame(X_test)
test_y = pd.DataFrame(Y_test)

train = pd.concat([train_x, train_y], axis = 1)
test = pd.concat([test_x, test_y], axis = 1)

train.to_csv('/Users/iris_train.csv', index = False)
test.to_csv('/Users/iris_test.csv', index = False)
```
[train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)

## 5.3 Fit a logistic regression model.

```{python, echo=3:17, fig.width = 5}
import pandas as pd
import numpy as np
# Notice we are using a new data set that needs to be read into the 
# environment 
tips = pd.read_csv('/Users/tips.csv')

# The following code is used to determine if the individual left more 
# than a 15% tip 
tips["fifteen"] = 0.15 * tips["total_bill"]
tips["greater15"] = np.where(tips["tip"] > tips["fifteen"], 1, 0)

import statsmodels.api as sm

# Notice the syntax of greater15 as a function of total_bill 
res = sm.formula.glm("greater15 ~ total_bill", family=sm.families.Binomial(), 
                     data=tips).fit()
print(res.summary())
```
A logistic regression model can be implemented using [sklearn](#SKLEARN), however [statsmodels.api](http://www.statsmodels.org/stable/glm.html#technical-documentation) provides a helpful summary about the model, so it is preferable for this example.

## 5.4 Fit a linear regression model on training data and assess against testing data.

```{python, echo=3:23}
import pandas as pd
import numpy as np
# Notice we are using new data sets that need to be read into the environment 
train = pd.read_csv('/Users/tips_train.csv')
test = pd.read_csv('/Users/tips_test.csv')

# Fit a linear regression model of tip by total_bill on the training data 
from sklearn import linear_model
regr = linear_model.LinearRegression()
# If your data has one feature, you need to reshape the 1D array
model = regr.fit(train["total_bill"].reshape(-1,1), train["tip"])

# Predict the tip based on the total_bill given in the testing data 
prediction = pd.DataFrame()
prediction["tip_hat"] = regr.predict(test["total_bill"].reshape(-1,1))

# Compute the squared difference between predicted tip and actual tip 
prediction["diff"] = (prediction["tip_hat"] - test["tip"])**2

# Compute the mean of the squared differences (mean squared error) 
# as an assessment of the model
mean_sq_error = np.mean(prediction["diff"])
print(mean_sq_error)
```
[LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)

## 5.5 Fit a decision tree model on training data and assess against testing data.

### a) Build a model, assess the model against the training data, plot the tree, and determine variable importance.

```{python, echo=3:19}
import pandas as pd
import numpy as np
# Notice we are using new data sets that need to be read into the environment 
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')

from sklearn import tree

# random_state is used to specify a seed for a random integer so that the 
# results are reproducible
clf = tree.DecisionTreeClassifier(criterion='entropy', random_state=29)
clf = clf.fit(train.drop(["Target"], axis = 1), train["Target"])

# Prediction on training data
scored = pd.DataFrame(clf.predict(train.drop(["Target"], axis = 1)))
scored["Target"] = train["Target"]

# Determine how many were correctly classified 
scored["correct"] = (scored["Target"] == scored[0])
print(pd.crosstab(index=scored["correct"], columns="count"))
```

Output:
![output](DT1.png)

```{python, echo=8:14}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')
from sklearn import tree
clf = tree.DecisionTreeClassifier(criterion='entropy', random_state=29)
clf = clf.fit(train.drop(["Target"], axis = 1), train["Target"])
# Determine variable importance
var_import = clf.feature_importances_
var_import = pd.DataFrame(var_import)
var_import = var_import.rename(columns = {0:'Importance'})
var_import = var_import.sort_values(by="Importance", kind = "mergesort", 
                                    ascending = False)
print(var_import.head())
```

### b) Assess the model against the testing data.

```{python, echo=8:14}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')
from sklearn import tree
clf = tree.DecisionTreeClassifier(criterion='entropy', random_state=29)
clf = clf.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
scored = pd.DataFrame(clf.predict(test.drop(["Target"], axis = 1)))
scored["Target"] = test["Target"]

# Determine how many were correctly classified
scored["correct"] = (scored["Target"] == scored[0])
print(pd.crosstab(index=scored["correct"], columns="count"))
```
[DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)

## 5.6 Fit a random forest classification model on training data and assess against testing data.

### a) Build a model, assess the model against the training data, and determine variable importance.

```{python, echo = 3:18}
import pandas as pd
import numpy as np
# Notice we are using new data sets that need to be read into the environment 
train = pd.read_csv('/Users/iris_train.csv')
test = pd.read_csv('/Users/iris_test.csv')

from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(random_state=29)
clf = clf.fit(train.drop(["Target"], axis = 1), train["Target"])

# Prediction on training data
scored = pd.DataFrame(clf.predict(train.drop(["Target"], axis = 1)))
scored["Target"] = train["Target"]

# Determine how many were correctly classified
scored["correct"] = (scored["Target"] == scored[0])
print(pd.crosstab(index=scored["correct"], columns="count"))
```

--

```{python, echo = 8:14}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/iris_train.csv')
test = pd.read_csv('/Users/iris_test.csv')
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(random_state=29)
clf = clf.fit(train.drop(["Target"], axis = 1), train["Target"])
# Determine variable importance
var_import = clf.feature_importances_
var_import = pd.DataFrame(var_import)
var_import = var_import.rename(columns = {0:'Importance'})
var_import = var_import.sort_values(by="Importance", kind = "mergesort", 
                                    ascending = False)
print(var_import.head())
```

### b) Assess the model against the testing data.

```{python, echo = 8:14}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/iris_train.csv')
test = pd.read_csv('/Users/iris_test.csv')
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(random_state=29)
clf = clf.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
scored = pd.DataFrame(clf.predict(test.drop(["Target"], axis = 1)))
scored["Target"] = test["Target"]

# Determine how many were correctly classified
scored["correct"] = (scored["Target"] == scored[0])
print(pd.crosstab(index=scored["correct"], columns="count"))
```
[RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)

## 5.7 Fit a random forest regression model on training data and assess against testing data.

### a) Build a model and assess the model against the training data.

```{python, echo = 3:19}
import pandas as pd
import numpy as np
# Notice we are re-using data sets but it is good to re-read the original 
# version back into the environment
train = pd.read_csv('/Users/tips_train.csv')
test = pd.read_csv('/Users/tips_test.csv')

from sklearn.ensemble import RandomForestRegressor

clf = RandomForestRegressor(random_state=29)
clf = clf.fit(train.drop(["tip"], axis = 1), train["tip"])

# Prediction on training data
scored = pd.DataFrame(clf.predict(train.drop(["tip"], axis = 1)))
scored["Target"] = train["tip"]

# Determine mean squared error
scored["diff"] = (scored["Target"] - scored[0])**2
print(scored["diff"].mean())
```

### b) Assess the model against the testing data.

```{python, echo = 8:14}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/tips_train.csv')
test = pd.read_csv('/Users/tips_test.csv')
from sklearn.ensemble import RandomForestRegressor
clf = RandomForestRegressor(random_state=29)
clf = clf.fit(train.drop(["tip"], axis = 1), train["tip"])
# Prediction on testing data
scored = pd.DataFrame(clf.predict(test.drop(["tip"], axis = 1)))
scored["Target"] = test["tip"]

# Determine mean squared error
scored["diff"] = (scored["Target"] - scored[0])**2
print(scored["diff"].mean())
```
[RandomForestRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)

## 5.8 Fit a gradient boosting model on training data and assess against testing data.

### a) Build a model and assess the model against the training data.

```{python, echo = 3:26}
import pandas as pd
import numpy as np
# Notice we are re-using data sets but it is good to re-read the original 
# version back into the environment
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')

from sklearn.ensemble import GradientBoostingClassifier

# n_estimators = total number of trees to fit which is analogous to the 
# number of iterations
# learning_rate = shrinkage or step-size reduction, whereas a lower 
# learning rate requires more iterations
# min_samples_leaf = minimum number of observations in the trees 
# terminal nodes

clf = GradientBoostingClassifier(random_state = 29, learning_rate = .01, min_samples_leaf = 20, n_estimators = 2500)
clf = clf.fit(train.drop(["Target"], axis = 1), train["Target"])

# Prediction on training data
scored = pd.DataFrame(clf.predict(train.drop(["Target"], axis = 1)))
scored["Target"] = train["Target"]

# Determine how many were correctly classified
scored["correct"] = (scored["Target"] == scored[0])
print(pd.crosstab(index = scored["correct"], columns = "count"))
```

### b) Assess the model against the testing data.

```{python, echo = 8:13}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')
from sklearn.ensemble import GradientBoostingClassifier
clf = GradientBoostingClassifier(random_state = 29, learning_rate = .01, min_samples_leaf = 20, n_estimators = 2500)
clf = clf.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
scored = pd.DataFrame(clf.predict(test.drop(["Target"], axis = 1)))
scored["Target"] = test["Target"]

# Determine how many were correctly classified
scored["correct"] = (scored["Target"] == scored[0])
print(pd.crosstab(index = scored["correct"], columns = "count"))
```
[GradientBoostingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)

## 5.9 Fit a support vector classification model.

### a) Build a model and assess the model against the training data.
```{python, echo = 3:35}
import pandas as pd
import numpy as np
# Notice we are re-using data sets but it is good to re-read the original 
# version back into the environment
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')

# First we need to scale the data
from sklearn.preprocessing import StandardScaler

train_features = train.drop(["Target"], axis = 1)
scaler = StandardScaler().fit(np.array(train_features))
train_scaled = scaler.transform(np.array(train_features))
train_scaled = pd.DataFrame(train_scaled)
train_scaled["Target"] = train["Target"]

test_features = test.drop(["Target"], axis = 1)
scaler = StandardScaler().fit(np.array(test_features))
test_scaled = scaler.transform(np.array(test_features))
test_scaled = pd.DataFrame(test_scaled)
test_scaled["Target"] = test["Target"]

# Fit a support vector classification model
from sklearn.svm import SVC
clf = SVC(random_state = 29, kernel = 'linear')
clf = clf.fit(train_scaled.drop(["Target"], axis = 1), train_scaled["Target"])

# Evaluation on training data
predictions = pd.DataFrame()
predictions["predY"] = clf.predict(train_scaled.drop(["Target"], axis = 1))

# Determine how many were correctly classified
predictions["actual"] = train_scaled["Target"]
predictions["correct"] = (predictions["actual"] == predictions["predY"])
print(pd.crosstab(index = predictions["correct"], columns = "count"))
```

### b) Assess the model against the testing data.
```{python, echo = 19:26}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')
from sklearn.preprocessing import StandardScaler
train_features = train.drop(["Target"], axis = 1)
scaler = StandardScaler().fit(np.array(train_features))
train_scaled = scaler.transform(np.array(train_features))
train_scaled = pd.DataFrame(train_scaled)
train_scaled["Target"] = train["Target"]
test_features = test.drop(["Target"], axis = 1)
scaler = StandardScaler().fit(np.array(test_features))
test_scaled = scaler.transform(np.array(test_features))
test_scaled = pd.DataFrame(test_scaled)
test_scaled["Target"] = test["Target"]
from sklearn.svm import SVC
clf = SVC(random_state = 29, kernel = 'linear')
clf = clf.fit(train_scaled.drop(["Target"], axis = 1), train_scaled["Target"])
# Evaluation on testing data
predictions = pd.DataFrame()
predictions["predY"] = clf.predict(test_scaled.drop(["Target"], axis = 1))

# Determine how many were correctly classified
predictions["actual"] = test_scaled["Target"]
predictions["correct"] = (predictions["actual"] == predictions["predY"])
print(pd.crosstab(index = predictions["correct"], columns = "count"))
```

## 5.10 Fit a support vector regression model.

### a) Generate random data based on a sine curve.
```{python, echo = 4:15, eval = FALSE}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# Generate the time variable
t = np.linspace(start = 0, stop = 0.5*np.pi, num = 100)

# Generate the sine curve with uniform noise
y1 = 5*np.sin(3*t) + np.random.uniform(size=100)

# Create a data frame for the generated data
random_data = pd.DataFrame()
random_data["X"] = t
random_data["Y"] = y1

# Plot the generated data
plt.scatter(t,y1)
plt.show()
```

Output:
![output](sinecurve.png)

[np.linspace()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html)
[np.sin()](https://docs.scipy.org/doc/numpy-1.10.4/reference/generated/numpy.sin.html)
[np.random.uniform()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.uniform.html)


### b) Fit a support vector regression model to the data.
```{python, echo = 8:12}
import pandas as pd
import numpy as np
t = np.linspace(start = 0, stop = 0.5*np.pi, num = 100)
y1 = 5*np.sin(3*t) + np.random.uniform(size=100)
random_data = pd.DataFrame()
random_data["X"] = t
random_data["Y"] = y1
from sklearn.svm import SVR
clf = SVR()
clf = clf.fit(random_data["X"].reshape(-1,1),random_data["Y"])
predictions = pd.DataFrame()
predictions["predY"] = clf.predict(random_data["X"].reshape(-1,1))
```

```{python, eval = FALSE}
plt.scatter(t,y1,color="black")
plt.scatter(t,predictions["predY"],color="r")
plt.show()
```

Output:
![output](sinecurve2.png)

[SVR](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)

```{python, echo = 13:15}
import pandas as pd
import numpy as np
t = np.linspace(start = 0, stop = 0.5*np.pi, num = 100)
y1 = 5*np.sin(3*t) + np.random.uniform(size=100)
random_data = pd.DataFrame()
random_data["X"] = t
random_data["Y"] = y1
from sklearn.svm import SVR
clf = SVR()
clf = clf.fit(random_data["X"].reshape(-1,1),random_data["Y"])
predictions = pd.DataFrame()
predictions["predY"] = clf.predict(random_data["X"].reshape(-1,1))
predictions["actual"] = random_data["Y"]
predictions["sq_diff"] = (predictions["predY"] - predictions["actual"])**2
print(predictions["sq_diff"].mean())
```

### c) Fit a linear regression model to the data.
```{python, echo = 8:12}
import pandas as pd
import numpy as np
t = np.linspace(start = 0, stop = 0.5*np.pi, num = 100)
y1 = 5*np.sin(3*t) + np.random.uniform(size=100)
random_data = pd.DataFrame()
random_data["X"] = t
random_data["Y"] = y1
from sklearn import linear_model
linMod = linear_model.LinearRegression()
linMod = linMod.fit(random_data["X"].reshape(-1,1),  random_data["Y"])
predictions = pd.DataFrame()
predictions["predY"] = linMod.predict(random_data["X"].reshape(-1,1))
```

```{python, eval = FALSE}
plt.scatter(t,y1,color="black")
plt.scatter(t,predictions["predY"],color="r")
plt.show()
```

Output:
![output](sinecurve3.png)

[LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)

```{python, echo = 13:15}
import pandas as pd
import numpy as np
t = np.linspace(start = 0, stop = 0.5*np.pi, num = 100)
y1 = 5*np.sin(3*t) + np.random.uniform(size=100)
random_data = pd.DataFrame()
random_data["X"] = t
random_data["Y"] = y1
from sklearn import linear_model
linMod = linear_model.LinearRegression()
linMod = linMod.fit(random_data["X"].reshape(-1,1),  random_data["Y"])
predictions = pd.DataFrame()
predictions["predY"] = linMod.predict(random_data["X"].reshape(-1,1))
predictions["actual"] = random_data["Y"]
predictions["sq_diff"] = (predictions["predY"] - predictions["actual"])**2
print(predictions["sq_diff"].mean())
```

***

# 6 Model Evaluation & Selection

## 6.1 Evaluate the accuracy of regression models.

## a) Evaluation on training data.
```{python, echo=3:19}
import pandas as pd
import numpy as np
# Notice we are re-using data sets but it is good to re-read the original 
# version back into the environment
train = pd.read_csv('/Users/tips_train.csv')
test = pd.read_csv('/Users/tips_test.csv')

# 1. Linear Regression Model
from sklearn.metrics import r2_score
from sklearn import linear_model
linMod = linear_model.LinearRegression()
linMod = linMod.fit(train.drop(["tip"], axis = 1),  train["tip"])

# Evaluation on training data
pred_lin = linMod.predict(train.drop(["tip"], axis = 1))

# Determine coefficient of determination score
r2_lin = r2_score(train["tip"], pred_lin)
print("Linear regression model r^2 score (coefficient of determination): %f" % r2_lin)
```

--

```{python, echo=6:16}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/tips_train.csv')
test = pd.read_csv('/Users/tips_test.csv')
from sklearn.metrics import r2_score
# 2. Random Forest Regression Model
from sklearn.ensemble import RandomForestRegressor
rfMod = RandomForestRegressor(random_state=29)
rfMod = rfMod.fit(train.drop(["tip"], axis = 1), train["tip"])

# Evaluation on training data
pred_rf = rfMod.predict(train.drop(["tip"], axis = 1))

# Determine coefficient of determination score
r2_rf = r2_score(train["tip"], pred_rf)
print("Random forest regression model r^2 score (coefficient of determination): %f" % r2_rf)
```

## b) Evaluation on testing data.
```{python, echo=9:16}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/tips_train.csv')
test = pd.read_csv('/Users/tips_test.csv')
from sklearn.metrics import r2_score
from sklearn import linear_model
linMod = linear_model.LinearRegression()
linMod = linMod.fit(train.drop(["tip"], axis = 1),  train["tip"])
# 1. Linear Regression Model (linMod)

# Evaluation on testing data
pred_lin = linMod.predict(test.drop(["tip"], axis = 1))

# Determine coefficient of determination score
r2_lin = r2_score(test["tip"], pred_lin)
print("Linear regression model r^2 score (coefficient of determination): %f" % r2_lin)
```

--

```{python, echo=9:16}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/tips_train.csv')
test = pd.read_csv('/Users/tips_test.csv')
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
rfMod = RandomForestRegressor(random_state=29)
rfMod = rfMod.fit(train.drop(["tip"], axis = 1), train["tip"])
# 2. Random Forest Regression Model (rfMod)

# Evaluation on testing data
pred_rf = rfMod.predict(test.drop(["tip"], axis = 1))

# Determine coefficient of determination score
r2_rf = r2_score(test["tip"], pred_rf)
print("Random forest regression model r^2 score (coefficient of determination): %f" % r2_rf)
```
The sklearn metric [r2_score](http://scikit-learn.org/stable/modules/model_evaluation.html#r2-score-the-coefficient-of-determination) is only one option for assessing a regression model.  Please go [here](http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics) for more information about other sklearn regression metrics.

## 6.2 Evaluate the accuracy of classification models.

### a) Evaluation on training data.

```{python, echo = 3:20}
import pandas as pd
import numpy as np
# Notice we are re-using data sets but it is good to re-read the original 
# version back into the environment
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')

# 1. Decision Tree Classification Model
from sklearn import tree
from sklearn.metrics import accuracy_score
treeMod = tree.DecisionTreeClassifier(criterion='entropy', random_state=29)
treeMod = treeMod.fit(train.drop(["Target"], axis = 1), train["Target"])

# Evaluation on training data
scored = pd.DataFrame(treeMod.predict(train.drop(["Target"], axis = 1)))
scored["Target"] = train["Target"]

# Determine accuracy score
accuracy_tree = accuracy_score(scored["Target"], scored[0])
print("Decision tree model accuracy: %f" % accuracy_tree)
```

--

```{python, echo = 5:17}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')
# 2. Random Forest Classification Model
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
rfMod = RandomForestClassifier(random_state=29)
rfMod = rfMod.fit(train.drop(["Target"], axis = 1), train["Target"])

# Evaluation on training data
scored = pd.DataFrame(rfMod.predict(train.drop(["Target"], axis = 1)))
scored["Target"] = train["Target"]

# Determine accuracy score
accuracy_rf = accuracy_score(scored["Target"], scored[0])
print("Random forest model accuracy: %f" % accuracy_rf)
```

--

```{python, echo = 5:17}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')
# 3. Gradient Boosting Classifcation Model
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score
gbmMod = GradientBoostingClassifier(random_state = 29, learning_rate = .01, min_samples_leaf = 20, n_estimators = 2500)
gbmMod = gbmMod.fit(train.drop(["Target"], axis = 1), train["Target"])

# Evaluation on training data
scored = pd.DataFrame(gbmMod.predict(train.drop(["Target"], axis = 1)))
scored["Target"] = train["Target"]

# Determine accuracy score
accuracy_gbm = accuracy_score(scored["Target"], scored[0])
print("Gradient boosting model accuracy: %f" % accuracy_gbm)
```

### b) Evaluation on testing data.

```{python, echo = 9:17}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')
from sklearn import tree
from sklearn.metrics import accuracy_score
treeMod = tree.DecisionTreeClassifier(criterion='entropy', random_state=29)
treeMod = treeMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# 1. Decision Tree Classification Model (treeMod)

# Evaluation on testing data
scored = pd.DataFrame(treeMod.predict(test.drop(["Target"], axis = 1)))
scored["Target"] = test["Target"]

# Determine accuracy score
accuracy_tree = accuracy_score(scored["Target"], scored[0])
print("Decision tree model accuracy: %f" % accuracy_tree)
```

--

```{python, echo = 9:17}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
rfMod = RandomForestClassifier(random_state=29)
rfMod = rfMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# 2. Random Forest Classification Model (rfMod)

# Evaluation on testing data
scored = pd.DataFrame(rfMod.predict(test.drop(["Target"], axis = 1)))
scored["Target"] = test["Target"]

# Determine accuracy score
accuracy_rf = accuracy_score(scored["Target"], scored[0])
print("Random forest model accuracy: %f" % accuracy_rf)
```

--

```{python, echo = 9:17}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score
gbmMod = GradientBoostingClassifier(random_state = 29, learning_rate = .01, min_samples_leaf = 20, n_estimators = 2500)
gbmMod = gbmMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# 3. Gradient Boosting Classifcation Model (gbmMod)

# Evaluation on testing data
scored = pd.DataFrame(gbmMod.predict(test.drop(["Target"], axis = 1)))
scored["Target"] = test["Target"]

# Determine accuracy score
accuracy_gbm = accuracy_score(scored["Target"], scored[0])
print("Gradient boosting model accuracy: %f" % accuracy_gbm)
```
Note: The sklearn metric [accuracy_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) is only one option for assessing a classification model.  Please go [here](http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) for more information about other sklearn classification metrics.

## 6.3 Evaluation with cross validation.

### a) KFold
```{python, echo = 3:18}
import pandas as pd
import numpy as np
# Notice we are using a new data set that need to be read into the 
# environment 
breastcancer = pd.read_csv('/Users/breastcancer.csv')

from sklearn import model_selection
from sklearn.ensemble import RandomForestClassifier

X = breastcancer.drop(["Target"], axis = 1)
Y = breastcancer["Target"]

kfold = model_selection.KFold(n_splits = 5, random_state = 29)
model = RandomForestClassifier(random_state = 29)
results = model_selection.cross_val_score(model, X, Y, cv = kfold)

print("Accuracy: %.2f%% +/- %.2f%%" % (results.mean()*100, 
                                       results.std()*100))
```

### b) ShuffleSplit

```{python, echo = 8:13}
import pandas as pd
import numpy as np
breastcancer = pd.read_csv('/Users/breastcancer.csv')
from sklearn import model_selection
from sklearn.ensemble import RandomForestClassifier
X = breastcancer.drop(["Target"], axis = 1)
Y = breastcancer["Target"]
shuffle = model_selection.ShuffleSplit(n_splits = 5, random_state = 29)
model = RandomForestClassifier(random_state = 29)
results = model_selection.cross_val_score(model, X, Y, cv = shuffle)

print("Accuracy: %.2f%% +/- %.2f%%" % (results.mean()*100, 
                                       results.std()*100))
```

***

\newpage

# Appendix

## 1 Built-in Python Data Types

* [Boolean](#Bool)

### Numeric types:
* [int](#int)
* [long](#LONG)
* [float](#float)
* [complex](#complex)

### Sequences:
* [str](#str)
* [bytes](#BYTE)
* [byte array](#BYTE)
* [list](#LIST)
* [tuple](#LIST)

### Sets:
* [set](#SET)
* [frozen set](#SET)

### Mapping:
* [dictionary](#dict)

## 2 Python Plotting Packages

### [Bokeh](#bokeh)
### [PyPlot](#PYPLOT)
### [Seaborn](#SEABORN)

***

\newpage

# Alphabetical Index

## Array

A NumPy array is a data type implemented by the [NumPy](#NUMPY) package in which the elements of the array are all of the same type.  Please see the following example of array creation and access:

```{python}
import numpy as np
my_array = np.array([1, 2, 3, 4])
print(my_array)
```

```{python, echo=3}
import numpy as np
my_array = np.array([1, 2, 3, 4])
print(my_array[3])
```

For more information, please see [NumPy Arrays](https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html).

***

## Bokeh {#bokeh}

[Bokeh](http://bokeh.pydata.org/en/latest/) is a Python package which is useful for interactive visualizations and is optimized for web browser presentations.

***

## Boolean {#Bool}

A [Boolean](https://docs.python.org/2/library/stdtypes.html#boolean-values) value is either True or False, and represents the truth of an expression or statement.  

***

## Bytes & Byte arrays {#BYTE}

A [byte](https://docs.python.org/3.1/library/functions.html#bytes) is a sequence of integers which is immutable, whereas a [byte array](https://docs.python.org/3.1/library/functions.html#bytearray) is its mutable counterpart.  

***

## complex {#complex}

A [complex number](https://docs.python.org/2/library/functions.html#complex) includes a real part and an imaginary part, both of which are floating point numbers.

***

## Data Frame {#DataFrame}

A [Pandas Data Frame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) is a two-dimensional tabular structure with labeled axes (rows and columns), where data observations are represented by rows and data variables are represented by columns.

***

## datetime

The [datetime](https://docs.python.org/2/library/datetime.html) Python module includes tools for manipulating data and time objects.

***

## Decimal {#DECIMAL}

[Decimal](https://docs.python.org/2/library/decimal.html) is a Python package which provides tools for decimal floating point arithmetic.

***

## Dictionary {#DICT}

A [dictionary](https://docs.python.org/2/tutorial/datastructures.html#dictionaries) is an associative array which is indexed by keys which map to values.  Therefore, a dictionary is an unordered set of key:value pairs where each key is unique.  Please see the following example of dictionary creation and access:

```{python}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
for_dict = pd.concat([student["Name"], student["Age"]], axis = 1)
class_dict = for_dict.set_index('Name').T.to_dict('list')
print(class_dict.get('James'))
```

***

## float {#float}

A [float](https://docs.python.org/2/library/functions.html#float) is a decimal point number.

***

## int {#int}

An [int](https://docs.python.org/3/library/functions.html#int) is a natural number.  In Python, you can convert to an int from a float by using the int() function.  Python stores ints with at least 32 bits of precision.

***

## List {#LIST}

A [list](https://www.tutorialspoint.com/python/python_lists.htm) is a sequence of comma-separated objects that need not be of the same type.  Please see the following example of list creation and access:

```{python}
list1 = ['item1', 102]
print(list1)
```

```{python, echo = 2}
list1 = ['item1', 102]
print(list1[1])
```

Python also has what are known as ["Tuples"](https://www.tutorialspoint.com/python/python_tuples.htm), which are immutable lists created in the same way as lists, except with paranthesis instead of brackets.

***

## Long {#LONG}

A [long](https://docs.python.org/2/library/functions.html#long) is a type of integer with unlimited precision.  In Python, you can convert to a long using the long() function.

***

## NumPy {#NUMPY}

[NumPy](http://www.numpy.org/) is a Python package which is useful for scientific and mathematical computing.

***

## pandas {#PANDAS}

[pandas](http://pandas.pydata.org/) is a Python package which is useful for working with data structures and performing data analysis.

***

## PyPlot {#PYPLOT}

[PyPlot](https://matplotlib.org/api/pyplot_api.html) is a Python package which is useful data plotting and visualization.

***

## Seaborn {#SEABORN}

[Seaborn](https://seaborn.pydata.org/) is another Python package which is useful for data plotting and visualization.  In particular, Seaborn includes tools for drawing attractive statistical graphics.

***

## Series

A [Pandas Series](https://pandas.pydata.org/pandas-docs/stable/dsintro.html) is a one-dimensional data frame, which is also called an array in R.  Please see the following example of Series creation and access:

```{python}
import pandas as pd
my_array = pd.Series([1, 3, 5, 9])
print(my_array)
```

```{python, echo = 3}
import pandas as pd
my_array = pd.Series([1, 3, 5, 9])
print(my_array[1])
```

***

## Sets & Frozen Sets {#SET}

A set is a unordered collection of immutable objects.  The difference between a [set and a frozen set](http://www.python-course.eu/sets_frozensets.php) is that the former is mutable, while the latter is immutable.  Please see the following example of set and frozen set creation and access:

```{python}
s = set(["1", "2", "3"])
print(s)
# s is a set, which means you can add or delete elements from s
```

```{python}
fs = frozenset(["1", "2", "3"])
print(fs)
# fs is a frozenset, which means you cannot add or delete elements from fs
```

***

## sklearn {#SKLEARN}

scikit-learn, or more commonly known as [sklearn](http://scikit-learn.org/stable/), is a Python package which is useful for basic and advanced data mining, machine learning, and data analysis.  sklearn includes tools for classification, regression, clustering, dimensionality reduction, model selection, and data pre-processing.

***

## str {#str}

[Strings](https://www.tutorialspoint.com/python/python_strings.htm) are a list of characters, though characters are not a type in Python, but rather a string of length 1.  Strings are indexable like arrays.  Please see the following example of String creation and access:

```{python}
s = 'My first string!'
print(s)
```

```{python, echo = 2}
s = 'My first string!'
print(s[5])
```

Please go [here](https://docs.python.org/3.1/library/functions.html#str) for more information on the str() function.

***

## sub {#SUB}

[sub](https://docs.python.org/2/library/re.html) is a function of the re Python package useful for replacing a pattern in a string.

***

For more information on Python packages and functions, along with helpful examples, please see [Python](https://www.python.org/).


