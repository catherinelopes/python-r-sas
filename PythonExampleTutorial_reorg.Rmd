---
title: "Python Tutorial"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, engine = 'python', engine.path="/Users/elaineek/anaconda/bin/python")
```

First, you need to import several important Python packages for data manipulation and scientific computing.  The [pandas](#PANDAS) package is helpful for data manipulation and the [NumPy](#NUMPY) package is helpful for scientific computing. 
```{python, eval = FALSE}
import pandas as pd
import numpy as np
```

In Python, comments are indicated in code with a "#" character, and arrays and matrices are zero-indexed.

# 1 Reading in Data and Basic Statistical Functions

## 1.1 Read in the data.

The following demonstrate importing data in Python given 3 different data formats.  The pandas package is able to read all 3 formats, as well as many others, using [Python IO tools](http://pandas.pydata.org/pandas-docs/version/0.20/io.html). 

### a) Read the data in as a .csv file.

```{python, eval = FALSE}
student = pd.read_csv('/Users/class.csv')
```

### b) Read the data in as a .xls file.

```{python, eval = FALSE}
# Notice you must specify the file location, as well as the name of the sheet 
# of the .xls file you want to import
student_xls = pd.read_excel(open('/Users/class.xls', 'rb'), 
                            sheetname='class')
```

### c) Read the data in as a .json file.

```{python, eval = FALSE}
student_json = pd.read_json('/Users/class.json')
```

## 1.2 Find the dimensions of the data set.

The dimensions of a [DataFrame](#DataFrame) in Python are known as an attribute of the object.  Therefore, you can state the data name followed by ".shape" to return the dimensions of the data.

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student.shape)
```

## 1.3 Find basic information about the data set.

Information about a [DataFrame](#DataFrame) is available by calling the ".info()" function on the data.

```{python, echo=3:4}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
# Notice that student is a DataFrame object
print(student.info())
```

## 1.4 Look at the first 5 observations.

The first 5 observations of a [DataFrame](#DataFrame) are available by calling the ".head()" function on the data.  By default, head() returns 5 observations.  To return the first *n* observations, pass the integer *n* into the function.  The tail() function is analogous and returns the last observations.

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student.head())
```

## 1.5 Calculate mean of numeric variables.

```{python, echo=3:5}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
# By default, the mean() function returns the mean of numeric variables of 
# the data only
print(student.mean())
```

## 1.6 Compute summary statistics of the data set.

Summary statistics of a [DataFrame](#DataFrame) are available by calling the ".describe()" function on the data.

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student.describe())
```

## 1.7 Descriptive statistics functions applied to variables of the data set.

```{python, echo=3:5}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
# Notice the subsetting of student with [] and the name of the variable in 
# quotes 
print(student["Weight"].std())
```

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student["Weight"].sum())
```

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student["Weight"].count())
```

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student["Weight"].max())
```

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student["Weight"].min())
```

```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(student["Weight"].median())
```

## 1.8 Produce a one-way table to describe the frequency of a variable.

### a) Produce a one-way table of a discrete variable.
```{python, echo=3:5}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
# columns = "count" indicates to make the descriptive portion of the table 
# the counts of each level of the index variable
print(pd.crosstab(index=student["Age"], columns="count"))
```

### b) Produce a one-way table of a categorical variable.
```{python, echo=3}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
print(pd.crosstab(index=student["Sex"], columns="count"))
```

## 1.9 Produce a two-way table to describe the frequency of two categorical or discrete variables.

```{python, echo=3:4}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
# Notice the specification of a variable for the columns argument, instead 
# of "count"
print(pd.crosstab(index=student["Age"], columns=student["Sex"]))
```
[crosstab()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html)

## 1.10 Select a subset of the data that meets a certain criterion.

```{python, echo=3:4}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
females = student.query('Sex == "F"')
print(females.head())
```
[query()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html)

## 1.11 Determine the correlation between two continuous variables.

```{python, echo = 3:5}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
# axis = 1 option indicates to concatenate column-wise
height_weight = pd.concat([student["Height"], student["Weight"]], axis = 1)
print(height_weight.corr(method = "pearson"))
```
[pd.concat()](http://pandas.pydata.org/pandas-docs/version/0.20/generated/pandas.concat.html)

[corr()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html)

***

\newpage

# 2 Basic Graphing and Plotting Functions

The Matplotlib [PyPlot](#PYPLOT) package is a standard Python package to use for plotting.  For more information on other Python plotting packages, please see the Appendix Section 2.

```{python}
import matplotlib.pyplot as plt
```

## 2.1 Visualize a single continuous variable by producing a histogram.

```{python, eval=FALSE}
# Notice how the bin endpoints are set so the histogram is the same
# as that produced by SAS and R
# Also notice the labeling of the axes
plt.hist(student["Weight"], bins=[40,60,80,100,120,140,160])
plt.xlabel('Weight')
plt.ylabel('Frequency')
plt.show()
```

Output:
![output](histogram2.png)

## 2.2 Visualize a single continuous variable by producing a boxplot.

```{python, eval=FALSE}
# showmeans=True tells Python to plot the mean of the variable on the boxplot 
plt.boxplot(student["Weight"], showmeans=True)
 # prevents Python from printing a "1" at the bottom of the boxplot
plt.xticks([]) # prevents Python from printing a "1" at the bottom of the boxplot
plt.ylabel('Weight')
plt.show()
```

Output:
![output](boxplot.png)

## 2.3 Visualize two continuous variables by producing a scatterplot.

```{python, eval=FALSE}
# Notice here you specify the x variable first followed by the y variable 
plt.scatter(student["Height"], student["Weight"])
plt.xlabel("Height")
plt.ylabel("Weight")
plt.show()
```

Output:
![output](scatter.png)

## 2.4 Visualize a relationship between two continuous variables by producing a scatterplot and a plotted line of best fit.

```{python, eval=FALSE}
x = student["Height"]
y = student["Weight"]

# np.polyfit() models Weight as a function of Height and returns the 
# parameters
m, b = np.polyfit(x, y, 1)
plt.scatter(x, y)

# plt.text() prints the equation of the line of best fit, with the first two 
# arguments specifying the x and y locations of the text, respectively 
# %f indicates to print a floating point number, that is specified following
# the string
plt.text(51, 140, "Line: y = %f x + %f"% (m,b))
plt.plot(x, m*x + b)
plt.xlabel("Height")
plt.ylabel("Weight")
plt.show()
```

Output:
![output](lineofbestfit.png)

[np.polyfit()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html)

## 2.5 Visualize a categorical variable by producing a bar chart.

```{python, eval=FALSE}
# Get the counts of Sex 
counts = pd.crosstab(index=student["Sex"], columns="count")

# len() returns the number of categories of Sex (2)
# np.arange() creates a vector of the specified length
num = np.arange(len(counts))
plt.bar(num,counts["count"], align='center', alpha=0.5)

# Set the xticks to be the indices of counts
plt.xticks(num, counts.index)
plt.xlabel("Sex")
plt.ylabel("Frequency")
plt.show()
```

Output:
![output](barchart.png)

[np.arange()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.arange.html)

## 2.6 Visualize a continuous variable, grouped by a categorical variable, by producing side-by-side boxplots.

### a) Simple side-by-side boxplot without color.
```{python, eval=FALSE}
# Subset data set to return only female weights, and then only male weights 
Weight_F = np.array(student.query('Sex == "F"')["Weight"])
Weight_M = np.array(student.query('Sex == "M"')["Weight"])
Weights = [Weight_F, Weight_M]

# PyPlot automatically plots the two weights side-by-side since Weights 
# is a 2D array
plt.boxplot(Weights, showmeans=True, labels=('F', 'M'))
plt.xlabel('Sex')
plt.ylabel('Weight')
plt.show()
```

Output:
![output](sboxplot.png)

[np.array](https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html)

### b) More advanced side-by-side boxplot with color.

```{python, eval=FALSE}
import seaborn as sns
sns.boxplot(x="Sex", y="Weight", hue="Sex", data = student, showmeans=True)
plt.show()
```
[seaborn boxplot](http://seaborn.pydata.org/generated/seaborn.boxplot.html)

[seaborn](#SEABORN)

Output:
![output](snsboxplot.png)

***

\newpage

# 3 Basic Data Wrangling and Manipulation

## 3.1 Create a new variable in a data set as a function of existing variables in the data set.

```{python, echo=3:7}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
# Notice here how you can create the BMI column in the 
# data set just by naming it 

student["BMI"] = student["Weight"] / student["Height"]**2 * 703
print(student.head())
```

## 3.2 Create a new variable in a data set using if/else logic of existing variables in the data set.

```{python, echo=5:8}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
# Notice the use of the np.where() function for a single condition 
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
print(student.head())
```

[np.where()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html)

## 3.3 Create new variables in a data set using mathematical functions applied to existing variables in the data set.

Using the log() function, the exp() function, the sqrt() function, and the abs() function.
```{python, echo=6:14}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
student["LogWeight"] = np.log(student["Weight"])
student["ExpAge"] = np.exp(student["Age"])
student["SqrtHeight"] = np.sqrt(student["Height"])
student["BMI Neg"] = np.where(student["BMI"] < 19.0, -student["BMI"], student["BMI"])
student["BMI Pos"] = np.abs(student["BMI Neg"])

# Create a boolean variable
student["BMI Check"] = (student["BMI Pos"] == student["BMI"])
print(student.head())
```

[np.log()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html), [np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html), [np.sqrt()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sqrt.html), [np.where()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html), [np.abs()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.absolute.html)

## 3.4 Drop variables from a data set.

```{python, echo=12:15}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
student["LogWeight"] = np.log(student["Weight"])
student["ExpAge"] = np.exp(student["Age"])
student["SqrtHeight"] = np.sqrt(student["Height"])
student["BMI Neg"] = np.where(student["BMI"] < 19.0, -student["BMI"], student["BMI"])
student["BMI Pos"] = np.abs(student["BMI Neg"])
student["BMI Check"] = (student["BMI Pos"] == student["BMI"])
# axis = 1 indicates to drop columns instead of rows
student = student.drop(["LogWeight", "ExpAge", "SqrtHeight", "BMI Neg", 
                        "BMI Pos", "BMI Check"], axis = 1)
print(student.head())
```
[drop()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html)

## 3.5 Sort a data set by a variable.

### a) Sort data set by a continuous variable.

```{python, echo=6:9}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
# Notice the kind="mergesort" which indicates to use a stable sorting 
# algorithm 
student = student.sort_values(by="Age", kind = "mergesort")
print(student.head())
```

### b) Sort data set by a categorical variable.

```{python, echo=7:9}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
student = student.sort_values(by="Age", kind = "mergesort")
student = student.sort_values(by="Sex", kind = "mergesort")
# Notice that the data is now sorted first by Sex and then within Sex by Age 
print(student.head())
```
[sort_values](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html)

## 3.6 Compute descriptive statistics of continuous variables, grouped by a categorical variable.

```{python, echo=8}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
student = student.sort_values(by="Age", kind = "mergesort")
student = student.sort_values(by="Sex", kind = "mergesort")
print(student.groupby(by = "Sex").mean())
```
[groupby()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html)

## 3.7 Add a new row to the bottom of a data set.

```{python, echo=8:9}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
student = student.sort_values(by="Age", kind = "mergesort")
student = student.sort_values(by="Sex", kind = "mergesort")
# Look at the tail of the data currently
print(student.tail())
```

```{python, echo=8:16}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
student = student.sort_values(by="Age", kind = "mergesort")
student = student.sort_values(by="Sex", kind = "mergesort")
student = student.append({'Name':'Jane', 'Sex':'F', 'Age':14, 'Height':56.3, 
                          'Weight':77.0, 'BMI':17.077695, 
                          'BMI Class': 'Underweight'}, 
                         ignore_index=True)

# Notice the change in the indices because of the ignore_index=True option 
# which allows for a Series, or one-dimensional DataFrame, to be appended 
# to an existing DataFrame 

print(student.tail())
```
[append()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.append.html)

## 3.8 Create a user defined function and apply it to a variable in the data set to create a new variable in the data set.

```{python, echo=12:16}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student["BMI"] = student["Weight"] / student["Height"]**2 * 703
student["BMI Class"] = np.where(student["BMI"] < 19.0, "Underweight", "Healthy")
student = student.sort_values(by="Age", kind = "mergesort")
student = student.sort_values(by="Sex", kind = "mergesort")
student = student.append({'Name':'Jane', 'Sex':'F', 'Age':14, 'Height':56.3, 
                          'Weight':77.0, 'BMI':17.077695, 
                          'BMI Class': 'Underweight'}, 
                         ignore_index=True)
def toKG(lb):
    return (0.45359237 * lb)

student["Weight KG"] = student["Weight"].apply(toKG)
print(student.head())
```
[apply()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html)

[user defined functions](https://www.tutorialspoint.com/python/python_functions.htm)

## 3.9 Caste a [Data Frame](#DataFrame) to a different object type.

```{python, echo=4}
import pandas as pd
import numpy as np
student = pd.read_csv('/Users/class.csv')
student_num = pd.concat([student["Age"], student["Height"], 
                         student["Weight"]], axis = 1)
student_float = student_num.astype(float)
print(student_float.head())
```
[astype()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.astype.html)

***

\newpage

# 4 More Advanced Data Wrangling

## 4.1 Drop observations with missing information.

```{python, echo=2:8}
import pandas as pd
# Notice the use of the fish data set because it has some missing 
# observations 
fish = pd.read_csv('/Users/fish.csv')

# First sort by Weight, requesting those with NA for Weight first 
fish = fish.sort_values(by='Weight', kind='mergesort', na_position='first')
print(fish.head())
```

--

```{python, echo=4:5}
import pandas as pd
fish = pd.read_csv('/Users/fish.csv')
fish = fish.sort_values(by='Weight', kind='mergesort', na_position='first')
new_fish = fish.dropna()
print(new_fish.head())
```
[dropna()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html)

## 4.2 Merge two data sets together on a common variable.

### a) First, select specific columns of a data set to create two smaller data sets.

```{python, echo=2:8}
import pandas as pd
# Notice the use of the student data set again, however we want to reload it
# without the changes we've made previously 
student = pd.read_csv('/Users/class.csv')
student1 = pd.concat([student["Name"], student["Sex"], student["Age"]], 
                    axis = 1)
print(student1.head())
```

--

```{python, echo=3:5}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
student2 = pd.concat([student["Name"], student["Height"], student["Weight"]], 
                    axis = 1)
print(student2.head())
```

### b) Second, we want to merge the two smaller data sets on the common variable.

```{python, echo=7:8}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
student1 = pd.concat([student["Name"], student["Sex"], student["Age"]], 
                    axis = 1)
student2 = pd.concat([student["Name"], student["Height"], student["Weight"]], 
                    axis = 1)
new = pd.merge(student1, student2, on="Name")
print(new.head())
```

### c) Finally, we want to check to see if the merged data set is the same as the original data set.

```{python, echo=8}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
student1 = pd.concat([student["Name"], student["Sex"], student["Age"]], 
                    axis = 1)
student2 = pd.concat([student["Name"], student["Height"], student["Weight"]], 
                    axis = 1)
new = pd.merge(student1, student2, on="Name")
print(student.equals(new))
```
[merge()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html)

## 4.3 Merge two data sets together by index number only.

### a) First, select specific columns of a data set to create two smaller data sets.

```{python, echo=3:5}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
newstudent1 = pd.concat([student["Name"], student["Sex"], student["Age"]], 
                    axis = 1)
print(newstudent1.head())
```

```{python, echo=3:4}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
newstudent2 = pd.concat([student["Height"], student["Weight"]], axis = 1)
print(newstudent2.head())
```

### b) Second, we want to join the two smaller data sets.

```{python, echo=7:8}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
newstudent1 = pd.concat([student["Name"], student["Sex"], student["Age"]], 
                    axis = 1)
newstudent2 = pd.concat([student["Height"], student["Weight"]], 
                    axis = 1)
new2 = newstudent1.join(newstudent2)
print(new2.head())
```

### c) Finally, we want to check to see if the joined data set is the same as the original data set.

```{python, echo=8}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
newstudent1 = pd.concat([student["Name"], student["Sex"], student["Age"]], 
                    axis = 1)
newstudent2 = pd.concat([student["Height"], student["Weight"]], 
                    axis = 1)
new2 = newstudent1.join(newstudent2)
print(student.equals(new2))
```
[join()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html)

## 4.4 Create a pivot table to summarize information about a data set.

```{python, echo=3:17}
import pandas as pd
import numpy as np
# Notice we are using a new data set that needs to be read into the 
# environment 
price = pd.read_csv('/Users/price.csv')

# The following code is used to remove the ',' and '$' characters from 
# the ACTUAL colum so that the values can be summed 
from re import sub
from decimal import Decimal
def trim_money(money):
    return(float(Decimal(sub(r'[^\d.]', '', money))))

price["REVENUE"] = price["ACTUAL"].apply(trim_money)
table = pd.pivot_table(price, index=["COUNTRY", "STATE", "PRODTYPE", 
                                     "PRODUCT"], values="REVENUE", aggfunc=np.sum)
print(table.head())
```
[sub](#SUB)

[Decimal](#DECIMAL) 

[pd.pivot_table()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.pivot_table.html) is similar to the [pd.pivot()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html) function

## 4.5 Return all unique values from a text variable.
```{python}
import pandas as pd
import numpy as np
price = pd.read_csv('/Users/price.csv')
print(np.unique(price["STATE"]))
```
[np.unique()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.unique.html)

***

\newpage

The following sections focus on the Python [sklearn](#SKLEARN) package.  

# 5 Preparation & Basic Regression

## 5.1 Pre-process a data set using principal component analysis.

```{python, echo = 3:12}
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA

# Notice we are using a new data set that needs to be read into the 
# environment 
iris = pd.read_csv('/Users/iris.csv')
features = iris.drop(["Target"], axis = 1)

pca = PCA(n_components = 4)
pca = pca.fit(features)
print(np.transpose(pca.components_))
```
[PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA) [pca.fit()]() [np.transpose()]()

## 5.2 Split data into training and testing data and export as a .csv file.

```{python, eval = FALSE}
from sklearn.model_selection import train_test_split

target = iris["Target"]

# The following code splits the iris data set into 70% train and 30% test
X_train, X_test, Y_train, Y_test = train_test_split(features, target, 
                                                    test_size = 0.3, 
                                                    random_state = 29)
train_x = pd.DataFrame(X_train)
train_y = pd.DataFrame(Y_train)
test_x = pd.DataFrame(X_test)
test_y = pd.DataFrame(Y_test)

train = pd.concat([train_x, train_y], axis = 1)
test = pd.concat([test_x, test_y], axis = 1)

train.to_csv('/Users/iris_train.csv', index = False)
test.to_csv('/Users/iris_test.csv', index = False)
```
[train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)

## 5.3 Fit a logistic regression model.

```{python, echo=3:17, fig.width = 5}
import pandas as pd
import numpy as np
# Notice we are using a new data set that needs to be read into the 
# environment 
tips = pd.read_csv('/Users/tips.csv')

# The following code is used to determine if the individual left more 
# than a 15% tip 
tips["fifteen"] = 0.15 * tips["total_bill"]
tips["greater15"] = np.where(tips["tip"] > tips["fifteen"], 1, 0)

import statsmodels.api as sm

# Notice the syntax of greater15 as a function of total_bill 
logreg = sm.formula.glm("greater15 ~ total_bill", family=sm.families.Binomial(), 
                     data=tips).fit()
print(logreg.summary())
```
A logistic regression model can be implemented using [sklearn](#SKLEARN), however [statsmodels.api](http://www.statsmodels.org/stable/glm.html#technical-documentation) provides a helpful summary about the model, so it is preferable for this example.

## 5.4 Fit a linear regression model.

```{python, echo=3:23}
import pandas as pd
import numpy as np
# Notice we are using new data sets that need to be read into the environment 
train = pd.read_csv('/Users/tips_train.csv')
test = pd.read_csv('/Users/tips_test.csv')

# Fit a linear regression model of tip by total_bill on the training data 
from sklearn import linear_model
regr = linear_model.LinearRegression()
# If your data has one feature, you need to reshape the 1D array
linreg = regr.fit(train["total_bill"].values.reshape(-1,1), train["tip"])
print(linreg)
```
[LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)

# 6 Supervised Machine Learning

## 6.1 Fit a logistic regression model on training data and assess against testing data.

### a) Fit a logistic regression model on training data.
```{python, echo=3:20}
import pandas as pd
import numpy as np
# Notice we are using new data sets that need to be read into the 
# environment 
train = pd.read_csv('/Users/tips_train.csv')
test = pd.read_csv('/Users/tips_test.csv')

# The following code is used to determine if the individual left more 
# than a 15% tip 
train["fifteen"] = 0.15 * train["total_bill"]
train["greater15"] = np.where(train["tip"] > train["fifteen"], 1, 0)
test["fifteen"] = 0.15 * test["total_bill"]
test["greater15"] = np.where(test["tip"] > test["fifteen"], 1, 0)

import statsmodels.api as sm

# Notice the syntax of greater15 as a function of total_bill 
logreg = sm.formula.glm("greater15 ~ total_bill", family=sm.families.Binomial(), 
                     data=train).fit()
print(logreg.summary())
```

### b) Assess the model against the testing data.
```{python, echo=12:19}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/tips_train.csv')
test = pd.read_csv('/Users/tips_test.csv')
train["fifteen"] = 0.15 * train["total_bill"]
train["greater15"] = np.where(train["tip"] > train["fifteen"], 1, 0)
test["fifteen"] = 0.15 * test["total_bill"]
test["greater15"] = np.where(test["tip"] > test["fifteen"], 1, 0)
import statsmodels.api as sm
logreg = sm.formula.glm("greater15 ~ total_bill", family=sm.families.Binomial(), 
                     data=train).fit()
# Predict on testing data
predictions = logreg.predict(test["total_bill"])
scored = pd.DataFrame()
scored["predY"] = np.where(predictions < 0.5, 0, 1)

# Determine how many were correctly classified 
scored["correct"] = (test["greater15"] == scored["predY"])
print(pd.crosstab(index=scored["correct"], columns="count"))
```

## 6.2 Fit a linear regression model on training data and assess against testing data.

### a) Fit a linear regression model on training data.
```{python, echo=3:12}
import pandas as pd
import numpy as np
# Notice we are using new data sets that need to be read into the environment 
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')

# Fit a linear regression model of tip by total_bill on the training data 
from sklearn import linear_model
regr = linear_model.LinearRegression()
# If your data has one feature, you need to reshape the 1D array
linreg = regr.fit(train.drop(["Target"], axis = 1), train["Target"])
print(linreg)
```

### b) Assess the model against the testing data.
```{python, echo=8:14}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')
from sklearn import linear_model
regr = linear_model.LinearRegression()
linreg = regr.fit(train.drop(["Target"], axis = 1), train["Target"])
# Predict on testing data
scored = pd.DataFrame()
scored["predY"] = linreg.predict(test.drop(["Target"], axis = 1))

# Determine mean squared error
scored["sq_diff"] = (scored["predY"] - test["Target"])**2
print(np.mean(scored["sq_diff"]))
```

## 6.3 Fit a decision tree model on training data and assess against testing data.

### a) Fit a decision tree classification model.

#### i) Fit a decision tree classification model on training data, plot the tree, and determine variable importance.
```{python, echo=3:12}
import pandas as pd
import numpy as np
# Notice we are using new data sets that need to be read into the environment 
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')

from sklearn import tree

# random_state is used to specify a seed for a random integer so that the 
# results are reproducible
treeMod = tree.DecisionTreeClassifier(criterion='entropy', random_state=29)
treeMod = treeMod.fit(train.drop(["Target"], axis = 1), train["Target"])
```

Output:
![output](DT1.png)

```{python, echo=8:14}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')
from sklearn import tree
treeMod = tree.DecisionTreeClassifier(criterion='entropy', random_state=29)
treeMod = treeMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Determine variable importance
var_import = treeMod.feature_importances_
var_import = pd.DataFrame(var_import)
var_import = var_import.rename(columns = {0:'Importance'})
var_import = var_import.sort_values(by="Importance", kind = "mergesort", 
                                    ascending = False)
print(var_import.head())
```

#### ii) Assess the model against the testing data.
```{python, echo=8:13}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')
from sklearn import tree
treeMod = tree.DecisionTreeClassifier(criterion='entropy', random_state=29)
treeMod = treeMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
scored = treeMod.predict(test.drop(["Target"], axis = 1))

# Determine how many were correctly classified
Results = np.where(test["Target"] == scored, "Correct", "Wrong")
print(pd.crosstab(index=Results, columns="count"))
```
[DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)

### b) Fit a decision tree regression model.

#### i) Fit a decision tree regression model on training data and determine variable importance.
```{python, echo=4:10}
import pandas as pd
import numpy as np
from sklearn import tree
# Notice we are re-using data sets but it is good to re-read the original
# versions back into the environment
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')

treeMod = tree.DecisionTreeRegressor(random_state=29)
treeMod = treeMod.fit(train.drop(["Target"], axis = 1), train["Target"])

# Determine variable importance
var_import = treeMod.feature_importances_
var_import = pd.DataFrame(var_import)
var_import = var_import.rename(columns = {0:'Importance'})
var_import = var_import.sort_values(by="Importance", kind = "mergesort", 
                                    ascending = False)
print(var_import.head())
```

#### ii) Assess the model against the testing data.
```{python, echo=8:14}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')
from sklearn import tree
treeMod = tree.DecisionTreeRegressor(random_state=29)
treeMod = treeMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
scored = pd.DataFrame()
scored["predY"] = treeMod.predict(test.drop(["Target"], axis = 1))

# Determine mean squared error
scored["sq_diff"] = (scored["predY"] - test["Target"])**2
print(np.mean(scored["sq_diff"]))
```

## 6.4 Fit a random forest model on training data and assess against testing data.

### a) Fit a random forest classification model.

#### i) Fit a random forest classification model on training data and determine variable importance.
```{python, echo = 3:19}
import pandas as pd
import numpy as np
# Notice we are re-using data sets but it is good to re-read the original
# versions back into the environment
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')

from sklearn.ensemble import RandomForestClassifier

rfMod = RandomForestClassifier(random_state=29)
rfMod = rfMod.fit(train.drop(["Target"], axis = 1), train["Target"])

# Determine variable importance
var_import = rfMod.feature_importances_
var_import = pd.DataFrame(var_import)
var_import = var_import.rename(columns = {0:'Importance'})
var_import = var_import.sort_values(by="Importance", kind = "mergesort", 
                                    ascending = False)
print(var_import.head())
```

#### ii) Assess the model against the testing data.
```{python, echo = 8:13}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')
from sklearn.ensemble import RandomForestClassifier
rfMod = RandomForestClassifier(random_state=29)
rfMod = rfMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
scored = rfMod.predict(test.drop(["Target"], axis = 1))

# Determine how many were correctly classified
Results = np.where(test["Target"] == scored, "Correct", "Wrong")
print(pd.crosstab(index=Results, columns="count"))
```
[RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)

### b) Fit a random forest regression model.

#### i) Fit a random forest regression model on training data and determine variable importance.
```{python, echo = 3:19}
import pandas as pd
import numpy as np
# Notice we are re-using data sets but it is good to re-read the original 
# version back into the environment
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')

from sklearn.ensemble import RandomForestRegressor

rfMod = RandomForestRegressor(random_state=29)
rfMod = rfMod.fit(train.drop(["Target"], axis = 1), train["Target"])

# Determine variable importance
var_import = rfMod.feature_importances_
var_import = pd.DataFrame(var_import)
var_import = var_import.rename(columns = {0:'Importance'})
var_import = var_import.sort_values(by="Importance", kind = "mergesort", 
                                    ascending = False)
print(var_import.head())
```

#### ii) Assess the model against the testing data.

```{python, echo = 8:14}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')
from sklearn.ensemble import RandomForestRegressor
rfMod = RandomForestRegressor(random_state=29)
rfMod = rfMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
scored = pd.DataFrame()
scored["predY"] = rfMod.predict(test.drop(["Target"], axis = 1))

# Determine mean squared error
scored["diff"] = (test["Target"] - scored["predY"])**2
print(scored["diff"].mean())
```
[RandomForestRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)

## 6.5 Fit a gradient boosting model on training data and assess against testing data.

### a) Fit a gradient boosting classification model.

#### i) Fit a gradient boosting classification model on training data and determine variable importance.
```{python, echo = 3:24}
import pandas as pd
import numpy as np
# Notice we are re-using data sets but it is good to re-read the original 
# version back into the environment
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')

from sklearn.ensemble import GradientBoostingClassifier

# n_estimators = total number of trees to fit which is analogous to the 
# number of iterations
# learning_rate = shrinkage or step-size reduction, whereas a lower 
# learning rate requires more iterations

gbMod = GradientBoostingClassifier(random_state = 29, learning_rate = .01, n_estimators = 2500)
gbMod = gbMod.fit(train.drop(["Target"], axis = 1), train["Target"])

# Determine variable importance
var_import = gbMod.feature_importances_
var_import = pd.DataFrame(var_import)
var_import = var_import.rename(columns = {0:'Importance'})
var_import = var_import.sort_values(by="Importance", kind = "mergesort", 
                                    ascending = False)
print(var_import.head())
```

#### ii) Assess the model against the testing data.
```{python, echo = 8:13}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')
from sklearn.ensemble import GradientBoostingClassifier
gbMod = GradientBoostingClassifier(random_state = 29, learning_rate = .01, n_estimators = 2500)
gbMod = gbMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
scored = gbMod.predict(test.drop(["Target"], axis = 1))

# Determine how many were correctly classified
Results = np.where(test["Target"] == scored, "Correct", "Wrong")
print(pd.crosstab(index=Results, columns="count"))
```
[GradientBoostingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)

### b) Fit a gradient boosting regression model.

#### i) Fit a gradient boosting regression model on training data and determine variable importance.
```{python, echo = 3:24}
import pandas as pd
import numpy as np
# Notice we are re-using data sets but it is good to re-read the original 
# version back into the environment
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')

from sklearn.ensemble import GradientBoostingRegressor

gbMod = GradientBoostingRegressor(random_state = 29, learning_rate = .01, n_estimators = 2500)
gbMod = gbMod.fit(train.drop(["Target"], axis = 1), train["Target"])

# Determine variable importance
var_import = gbMod.feature_importances_
var_import = pd.DataFrame(var_import)
var_import = var_import.rename(columns = {0:'Importance'})
var_import = var_import.sort_values(by="Importance", kind = "mergesort", 
                                    ascending = False)
print(var_import.head())
```

#### ii) Assess the model against the testing data.
```{python, echo = 8:13}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')
from sklearn.ensemble import GradientBoostingRegressor
gbMod = GradientBoostingRegressor(random_state = 29, learning_rate = .01, n_estimators = 2500)
gbMod = gbMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
scored = pd.DataFrame()
scored["predY"] = gbMod.predict(test.drop(["Target"], axis = 1))

# Determine mean squared error
scored["diff"] = (test["Target"] - scored["predY"])**2
print(scored["diff"].mean())
```
[GradientBoostingRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)

## 6.6 Fit an extreme gradient boosting model on training data and assess against testing data.

### a) Fit an extreme gradient boosting classification model on training data and assess against testing data.

#### i) Fit an extreme gradient boosting classification model on training data.
```{python, echo = 3:12}
import pandas as pd
import numpy as np
# Notice we are re-using data sets but it is good to re-read the original 
# version back into the environment
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')

import xgboost as xgb

# Fit XGBClassifier model on training data
xgbMod = xgb.XGBClassifier(seed = 29, learning_rate = 0.01, n_estimators = 2500)
xgbMod = xgbMod.fit(train.drop(["Target"], axis = 1), train["Target"])
```

#### ii) Assess the model against the testing data.
```{python, echo = 8:13}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')
import xgboost as xgb
xgbMod = xgb.XGBClassifier(seed = 29, learning_rate = 0.01, n_estimators = 2500)
xgbMod = xgbMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
scored = xgbMod.predict(test.drop(["Target"], axis = 1))

# Determine how many were correctly classified
Results = np.where(test["Target"] == scored, "Correct", "Wrong")
print(pd.crosstab(index=Results, columns="count"))
```

### b) Fit an extreme gradient boosting regression model on training data and assess against testing data.

#### i) Fit an extreme gradient boosting regression model on training data.
```{python, echo = 3:12}
import pandas as pd
import numpy as np
# Notice we are re-using data sets but it is good to re-read the original 
# version back into the environment
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')

import xgboost as xgb

# Fit XGBClassifier model on training data
xgbMod = xgb.XGBRegressor(seed = 29, learning_rate = 0.01, n_estimators = 2500)
xgbMod = xgbMod.fit(train.drop(["Target"], axis = 1), train["Target"])
```

#### ii) Assess the model against the testing data.
```{python, echo = 8:14}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')
import xgboost as xgb
xgbMod = xgb.XGBRegressor(seed = 29, learning_rate = 0.01, n_estimators = 2500)
xgbMod = xgbMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
scored = pd.DataFrame()
scored["predY"] = xgbMod.predict(test.drop(["Target"], axis = 1))

# Determine mean squared error
scored["diff"] = (test["Target"] - scored["predY"])**2
print(scored["diff"].mean())
```

## 6.7 Fit a support vector model on training data and assess against testing data.

### a) Fit a support vector classification model.

#### i) Fit a support vector classification model on training data.
```{python, echo = 3:27}
import pandas as pd
import numpy as np
# Notice we are re-using data sets but it is good to re-read the original 
# version back into the environment
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')

# First we need to scale the data
from sklearn.preprocessing import StandardScaler

train_features = train.drop(["Target"], axis = 1)
scaler = StandardScaler().fit(np.array(train_features))
train_scaled = scaler.transform(np.array(train_features))
train_scaled = pd.DataFrame(train_scaled)
train_scaled["Target"] = train["Target"]

test_features = test.drop(["Target"], axis = 1)
scaler = StandardScaler().fit(np.array(test_features))
test_scaled = scaler.transform(np.array(test_features))
test_scaled = pd.DataFrame(test_scaled)
test_scaled["Target"] = test["Target"]

# Fit a support vector classification model
from sklearn.svm import SVC
svMod = SVC(random_state = 29, kernel = 'linear')
svMod = svMod.fit(train_scaled.drop(["Target"], axis = 1), 
                  train_scaled["Target"])
```

#### ii) Assess the model against the testing data.
```{python, echo = 19:24}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')
from sklearn.preprocessing import StandardScaler
train_features = train.drop(["Target"], axis = 1)
scaler = StandardScaler().fit(np.array(train_features))
train_scaled = scaler.transform(np.array(train_features))
train_scaled = pd.DataFrame(train_scaled)
train_scaled["Target"] = train["Target"]
test_features = test.drop(["Target"], axis = 1)
scaler = StandardScaler().fit(np.array(test_features))
test_scaled = scaler.transform(np.array(test_features))
test_scaled = pd.DataFrame(test_scaled)
test_scaled["Target"] = test["Target"]
from sklearn.svm import SVC
svMod = SVC(random_state = 29, kernel = 'linear')
svMod = svMod.fit(train_scaled.drop(["Target"], axis = 1), train_scaled["Target"])
# Prediction on testing data
scored = svMod.predict(test_scaled.drop(["Target"], axis = 1))

# Determine how many were correctly classified
Results = np.where(test_scaled["Target"] == scored, "Correct", "Wrong")
print(pd.crosstab(index=Results, columns="count"))
```

### b) Fit a support vector regression model.

#### i) Fit a support vector regression model on training data.
```{python, 3:11}
import pandas as pd
import numpy as np
# Notice we are re-using data sets but it is good to re-read the original 
# version back into the environment
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')

from sklearn.preprocessing import StandardScaler 

train_features = train.drop(["Target"], axis = 1) 
scaler = StandardScaler().fit(np.array(train_features)) 
train_scaled = scaler.transform(np.array(train_features)) 
train_scaled = pd.DataFrame(train_scaled) 
train_scaled["Target"] = train["Target"]

test_features = test.drop(["Target"], axis = 1) 
scaler = StandardScaler().fit(np.array(test_features)) 
test_scaled = scaler.transform(np.array(test_features)) 
test_scaled = pd.DataFrame(test_scaled) 
test_scaled["Target"] = test["Target"]

# Fit a support vector regression model
from sklearn.svm import SVR
svMod = SVR()
svMod = svMod.fit(train_scaled.drop(["Target"], axis = 1), 
                  train_scaled["Target"])
```

#### ii) Assess the model against the testing data.
```{python, echo = 17:23}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')
from sklearn.preprocessing import StandardScaler 
train_features = train.drop(["Target"], axis = 1) 
scaler = StandardScaler().fit(np.array(train_features)) 
train_scaled = scaler.transform(np.array(train_features)) 
train_scaled = pd.DataFrame(train_scaled) 
train_scaled["Target"] = train["Target"]
test_features = test.drop(["Target"], axis = 1) 
scaler = StandardScaler().fit(np.array(test_features)) 
test_scaled = scaler.transform(np.array(test_features)) 
test_scaled = pd.DataFrame(test_scaled) 
test_scaled["Target"] = test["Target"]
from sklearn.svm import SVR
svMod = SVR()
svMod = svMod.fit(train_scaled.drop(["Target"], axis = 1), train_scaled["Target"])
# Prediction on testing data
scored = pd.DataFrame()
scored["predY"] = svMod.predict(test_scaled.drop(["Target"], axis = 1))

# Determine mean squared error
scored["diff"] = (test_scaled["Target"] - scored["predY"])**2
print(scored["diff"].mean())
```

[SVR](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)

## 6.8 Fit a neural network model on training data and assess against testing data.

### a) Fit a neural network classification model.

#### i) Fit a neural network classification model on training data.
```{python, echo=3:10}
import pandas as pd
import numpy as np
# Notice we are using new data sets that need to be read into the environment 
train = pd.read_csv('/Users/digits_train.csv')
test = pd.read_csv('/Users/digits_test.csv')

# Fit a neural network classification model on training data
from sklearn.neural_network import MLPClassifier
nnMod = MLPClassifier(max_iter = 200, hidden_layer_sizes=(100,), random_state = 29)
nnMod = nnMod.fit(train.drop(["Target"], axis = 1), train["Target"])
```

#### ii) Assess the model against the testing data.
```{python}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/digits_train.csv')
test = pd.read_csv('/Users/digits_test.csv')
from sklearn.neural_network import MLPClassifier
nnMod = MLPClassifier(max_iter = 200, hidden_layer_sizes=(100,), random_state = 29)
nnMod = nnMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Prediction on testing data
scored = nnMod.predict(test.drop(["Target"], axis = 1))

# Determine how many were correctly classified
Results = np.where(test["Target"] == scored, "Correct", "Wrong")
print(pd.crosstab(index=Results, columns="count"))
```
[MLPClassifier()](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) [confusion_matrix()](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)

### b) Fit a neural network regression model.

#### i) Fit a neural network regression model on training data.
```{python, echo = 3:24}
import pandas as pd
import numpy as np
# Notice we are re-using data sets but it is good to re-read the original 
# version back into the environment
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')

# Scale input data
from sklearn.preprocessing import StandardScaler

train_features = train.drop(["Target"], axis = 1)
scaler = StandardScaler().fit(np.array(train_features))
train_scaled = scaler.transform(np.array(train_features))
train_scaled = pd.DataFrame(train_scaled)

test_features = test.drop(["Target"], axis = 1)
scaler = StandardScaler().fit(np.array(test_features))
test_scaled = scaler.transform(np.array(test_features))
test_scaled = pd.DataFrame(test_scaled)

# Fit neural network regression model, dividing target by 50 for scaling
from sklearn.neural_network import MLPRegressor
nnMod = MLPRegressor(max_iter = 200, random_state = 29, solver = 'lbfgs')
nnMod = nnMod.fit(train_scaled, train["Target"] / 50)
```

#### ii) Assess the model against testing data.
```{python, echo = 17:23}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/boston_train.csv')
test = pd.read_csv('/Users/boston_test.csv')
from sklearn.preprocessing import StandardScaler
train_features = train.drop(["Target"], axis = 1)
scaler = StandardScaler().fit(np.array(train_features))
train_scaled = scaler.transform(np.array(train_features))
train_scaled = pd.DataFrame(train_scaled)
test_features = test.drop(["Target"], axis = 1)
scaler = StandardScaler().fit(np.array(test_features))
test_scaled = scaler.transform(np.array(test_features))
test_scaled = pd.DataFrame(test_scaled)
from sklearn.neural_network import MLPRegressor
nnMod = MLPRegressor(max_iter = 200, random_state = 29, solver = 'lbfgs')
nnMod = nnMod.fit(train_scaled, train["Target"] / 50)
# Prediction on testing data, remembering to multiply by 50
scored = pd.DataFrame()
scored["predY"] = nnMod.predict(test.drop(["Target"], axis = 1))*50

# Determine mean squared error
scored["diff"] = (test["Target"] - scored["predY"])**2
print(scored["diff"].mean())
```
[MLPRegressor()](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor)

***

# 7 Model Evaluation & Selection

## 7.1 Evaluate the accuracy of regression models.

## a) Evaluation on training data.
```{python, echo=3:20}
import pandas as pd
import numpy as np
# Notice we are re-using data sets but it is good to re-read the original 
# version back into the environment
train = pd.read_csv('/Users/tips_train.csv')
test = pd.read_csv('/Users/tips_test.csv')

from sklearn.metrics import r2_score

# Random Forest Regression Model
from sklearn.ensemble import RandomForestRegressor
rfMod = RandomForestRegressor(random_state=29)
rfMod = rfMod.fit(train.drop(["tip"], axis = 1), train["tip"])

# Evaluation on training data
pred_rf = rfMod.predict(train.drop(["tip"], axis = 1))

# Determine coefficient of determination score
r2_rf = r2_score(train["tip"], pred_rf)
print("Random forest regression model r^2 score (coefficient of determination): %f" % r2_rf)
```

## b) Evaluation on testing data.
```{python, echo=10:17}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/tips_train.csv')
test = pd.read_csv('/Users/tips_test.csv')
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
rfMod = RandomForestRegressor(random_state=29)
rfMod = rfMod.fit(train.drop(["tip"], axis = 1), train["tip"])
from sklearn.metrics import r2_score
# Random Forest Regression Model (rfMod)

# Evaluation on testing data
pred_rf = rfMod.predict(test.drop(["tip"], axis = 1))

# Determine coefficient of determination score
r2_rf = r2_score(test["tip"], pred_rf)
print("Random forest regression model r^2 score (coefficient of determination): %f" % r2_rf)
```
The sklearn metric [r2_score](http://scikit-learn.org/stable/modules/model_evaluation.html#r2-score-the-coefficient-of-determination) is only one option for assessing a regression model.  Please go [here](http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics) for more information about other sklearn regression metrics.

## 7.2 Evaluate the accuracy of classification models.

### a) Evaluation on training data.
```{python, echo = 3:19}
import pandas as pd
import numpy as np
# Notice we are re-using data sets but it is good to re-read the original 
# version back into the environment
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')

# Random Forest Classification Model
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
rfMod = RandomForestClassifier(random_state=29)
rfMod = rfMod.fit(train.drop(["Target"], axis = 1), train["Target"])

# Evaluation on training data
scored = rfMod.predict(train.drop(["Target"], axis = 1))

# Determine accuracy score
accuracy_rf = accuracy_score(train["Target"], scored)
print("Random forest model accuracy: %f" % accuracy_rf)
```

### b) Evaluation on testing data.
```{python, echo = 9:16}
import pandas as pd
import numpy as np
train = pd.read_csv('/Users/breastcancer_train.csv')
test = pd.read_csv('/Users/breastcancer_test.csv')
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
rfMod = RandomForestClassifier(random_state=29)
rfMod = rfMod.fit(train.drop(["Target"], axis = 1), train["Target"])
# Random Forest Classification Model (rfMod)

# Evaluation on testing data
scored = rfMod.predict(test.drop(["Target"], axis = 1))

# Determine accuracy score
accuracy_rf = accuracy_score(test["Target"], scored)
print("Random forest model accuracy: %f" % accuracy_rf)
```
Note: The sklearn metric [accuracy_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) is only one option for assessing a classification model.  Please go [here](http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) for more information about other sklearn classification metrics.

## 7.3 Evaluation with cross validation.

### a) KFold
```{python, echo = 3:18}
import pandas as pd
import numpy as np
# Notice we are using a new data set that need to be read into the 
# environment 
breastcancer = pd.read_csv('/Users/breastcancer.csv')

from sklearn import model_selection
from sklearn.ensemble import RandomForestClassifier

X = breastcancer.drop(["Target"], axis = 1)
Y = breastcancer["Target"]

kfold = model_selection.KFold(n_splits = 5, random_state = 29)
model = RandomForestClassifier(random_state = 29)
results = model_selection.cross_val_score(model, X, Y, cv = kfold)

print("Accuracy: %.2f%% +/- %.2f%%" % (results.mean()*100, 
                                       results.std()*100))
```

### b) ShuffleSplit

```{python, echo = 8:13}
import pandas as pd
import numpy as np
breastcancer = pd.read_csv('/Users/breastcancer.csv')
from sklearn import model_selection
from sklearn.ensemble import RandomForestClassifier
X = breastcancer.drop(["Target"], axis = 1)
Y = breastcancer["Target"]
shuffle = model_selection.ShuffleSplit(n_splits = 5, random_state = 29)
model = RandomForestClassifier(random_state = 29)
results = model_selection.cross_val_score(model, X, Y, cv = shuffle)

print("Accuracy: %.2f%% +/- %.2f%%" % (results.mean()*100, 
                                       results.std()*100))
```

# 8 Unsupervised Machine Learning

## 8.1 KMeans Clustering

```{python}
import pandas as pd
import numpy as np
iris = pd.read_csv('/Users/iris.csv')

iris["Species"] = np.where(iris["Target"] == 0, "Setosa", np.where(iris["Target"] == 1, "Versicolor", "Virginica"))

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters = 3, random_state = 29).fit(pd.concat([iris["PetalLength"], iris["PetalWidth"], iris["SepalLength"], iris["SepalWidth"]], axis = 1))

iris["labels"] = kmeans.labels_

print(pd.crosstab(index = iris["labels"], columns = iris["Species"]))
```

[KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)

## 8.2 Spectral Clustering

```{python}
import pandas as pd
import numpy as np
iris = pd.read_csv('/Users/iris.csv')
iris["Species"] = np.where(iris["Target"] == 0, "Setosa", np.where(iris["Target"] == 1, "Versicolor", "Virginica"))

from sklearn.cluster import SpectralClustering

spectral = SpectralClustering(n_clusters = 3, random_state = 29).fit(pd.concat([iris["PetalLength"], iris["PetalWidth"], iris["SepalLength"], iris["SepalWidth"]], axis = 1))

iris["labels"] = spectral.labels_

print(pd.crosstab(index = iris["labels"], columns = iris["Species"]))
```

## 8.3 Ward Hierarchical Clustering

```{python}
import pandas as pd
import numpy as np
iris = pd.read_csv('/Users/iris.csv')
iris["Species"] = np.where(iris["Target"] == 0, "Setosa", np.where(iris["Target"] == 1, "Versicolor", "Virginica"))

from sklearn.cluster import AgglomerativeClustering

aggl = AgglomerativeClustering(n_clusters = 3).fit(pd.concat([iris["PetalLength"], iris["PetalWidth"], iris["SepalLength"], iris["SepalWidth"]], axis = 1))

iris["labels"] = aggl.labels_

print(pd.crosstab(index = iris["labels"], columns = iris["Species"]))
```

### 8.4 DBSCAN

```{python}
import pandas as pd
import numpy as np
iris = pd.read_csv('/Users/iris.csv')
iris["Species"] = np.where(iris["Target"] == 0, "Setosa", np.where(iris["Target"] == 1, "Versicolor", "Virginica"))

from sklearn.cluster import DBSCAN

dbscan = DBSCAN().fit(pd.concat([iris["PetalLength"], iris["PetalWidth"], iris["SepalLength"], iris["SepalWidth"]], axis = 1))

iris["labels"] = dbscan.labels_

print(pd.crosstab(index = iris["labels"], columns = iris["Species"]))
```


***

\newpage

# Appendix

## 1 Built-in Python Data Types

* [Boolean](#Bool)

### Numeric types:
* [int](#int)
* [long](#LONG)
* [float](#float)
* [complex](#complex)

### Sequences:
* [str](#str)
* [bytes](#BYTE)
* [byte array](#BYTE)
* [list](#LIST)
* [tuple](#LIST)

### Sets:
* [set](#SET)
* [frozen set](#SET)

### Mapping:
* [dictionary](#dict)

## 2 Python Plotting Packages

### [Bokeh](#bokeh)
### [PyPlot](#PYPLOT)
### [Seaborn](#SEABORN)

***

\newpage

# Alphabetical Index

## [Array](https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html)

A NumPy array is a data type implemented by the [NumPy](#NUMPY) package in which the elements of the array are all of the same type.  Please see the following example of array creation and access:
```{python}
import numpy as np
my_array = np.array([1, 2, 3, 4])
print(my_array)
```
```{python, echo=3}
import numpy as np
my_array = np.array([1, 2, 3, 4])
print(my_array[3])
```

## [Bokeh](http://bokeh.pydata.org/en/latest/) {#bokeh}
A Python package which is useful for interactive visualizations and is optimized for web browser presentations.

## [Boolean](https://docs.python.org/2/library/stdtypes.html#boolean-values) {#Bool}
A value that is either True or False, and represents the truth of an expression or statement.  

## Bytes & Byte arrays {#BYTE}
A [byte](https://docs.python.org/3.1/library/functions.html#bytes) is a sequence of integers which is immutable, whereas a [byte array](https://docs.python.org/3.1/library/functions.html#bytearray) is its mutable counterpart.  

## [complex](https://docs.python.org/2/library/functions.html#complex) {#complex}
A number which includes a real part and an imaginary part, both of which are [floating point](#float) numbers.

## [Data Frame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) {#DataFrame}
A two-dimensional tabular structure with labeled axes (rows and columns), where data observations are represented by rows and data variables are represented by columns.

## [datetime](https://docs.python.org/2/library/datetime.html)
A Python module which includes tools for manipulating data and time objects.

## [Decimal](https://docs.python.org/2/library/decimal.html) {#DECIMAL}
A Python package which provides tools for decimal [floating point](#float) arithmetic.

## [Dictionary](https://docs.python.org/2/tutorial/datastructures.html#dictionaries) {#DICT}
An associative array which is indexed by keys which map to values.  Therefore, a dictionary is an unordered set of key:value pairs where each key is unique.  Please see the following example of dictionary creation and access:
```{python}
import pandas as pd
student = pd.read_csv('/Users/class.csv')
for_dict = pd.concat([student["Name"], student["Age"]], axis = 1)
class_dict = for_dict.set_index('Name').T.to_dict('list')
print(class_dict.get('James'))
```

## [float](https://docs.python.org/2/library/functions.html#float) {#float}
A decimal point number.

## [int](https://docs.python.org/3/library/functions.html#int) {#int}
A positive or negative natural number.  In Python, you can convert to an int from a float by using the int() function.  Python stores ints with at least 32 bits of precision.

## [List](https://www.tutorialspoint.com/python/python_lists.htm) {#LIST}
A sequence of comma-separated objects that need not be of the same type.  Please see the following example of list creation and access:
```{python}
list1 = ['item1', 102]
print(list1)
```
```{python, echo = 2}
list1 = ['item1', 102]
print(list1[1])
```

Python also has what are known as ["Tuples"](https://www.tutorialspoint.com/python/python_tuples.htm), which are immutable lists created in the same way as lists, except with paranthesis instead of brackets.

## [Long](https://docs.python.org/2/library/functions.html#long) {#LONG}
A type of integer with unlimited precision.  In Python, you can convert to a long using the long() function.

## [NumPy](http://www.numpy.org/) {#NUMPY}
A Python package which is useful for scientific and mathematical computing.

## [pandas](http://pandas.pydata.org/) {#PANDAS}
A Python package which is useful for working with data structures and performing data analysis.

## [PyPlot](https://matplotlib.org/api/pyplot_api.html) {#PYPLOT}
A Python package which is useful data plotting and visualization.

## [Seaborn](https://seaborn.pydata.org/) {#SEABORN}
A Python package which is useful for data plotting and visualization.  In particular, Seaborn includes tools for drawing attractive statistical graphics.

## [Series](https://pandas.pydata.org/pandas-docs/stable/dsintro.html)

A one-dimensional data frame.  Please see the following example of Series creation and access:
```{python}
import pandas as pd
my_array = pd.Series([1, 3, 5, 9])
print(my_array)
```
```{python, echo = 3}
import pandas as pd
my_array = pd.Series([1, 3, 5, 9])
print(my_array[1])
```

## Sets & Frozen Sets {#SET}
A set is a unordered collection of immutable objects.  The difference between a [set and a frozen set](http://www.python-course.eu/sets_frozensets.php) is that the former is mutable, while the latter is immutable.  Please see the following example of set and frozen set creation and access:
```{python}
s = set(["1", "2", "3"])
print(s)
# s is a set, which means you can add or delete elements from s
```
```{python}
fs = frozenset(["1", "2", "3"])
print(fs)
# fs is a frozenset, which means you cannot add or delete elements from fs
```

## [sklearn](http://scikit-learn.org/stable/) {#SKLEARN}
scikit-learn, or more commonly known as sklearn, is a Python package which is useful for basic and advanced data mining, machine learning, and data analysis.  sklearn includes tools for classification, regression, clustering, dimensionality reduction, model selection, and data pre-processing.

## [str](https://www.tutorialspoint.com/python/python_strings.htm) {#str}
A list of characters, though characters are not a type in Python, but rather a string of length 1.  Strings are indexable like arrays.  Please see the following example of String creation and access:
```{python}
s = 'My first string!'
print(s)
```
```{python, echo = 2}
s = 'My first string!'
print(s[5])
```

Please go [here](https://docs.python.org/3.1/library/functions.html#str) for more information on the str() function.

## [sub](https://docs.python.org/2/library/re.html) {#SUB}
A function of the re Python package useful for replacing a pattern in a string.

***

For more information on Python packages and functions, along with helpful examples, please see [Python](https://www.python.org/).


